{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier\n",
        "## CS 72 Final Project\n",
        "### John Guerrerio\n",
        "### john.j.guerrerio.26@dartmouth.edu\n",
        "\n",
        "This notebook contains the code to fine-tune a random forest classifier for the Inbox Guardian classification task.  We use a dataset of the most recent 500 email chains we recieved - see the write up for details.  "
      ],
      "metadata": {
        "id": "O3hMZpm-L1tz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqHHkvnXQHDZ"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_STATE = 42 # random seed to ensure results are reproducible\n",
        "META = False # True if we only want to show the model senders and subjects, false if we want to pass in email body as well\n",
        "THREE_CLASSES = True # True for trinary classification task, false for binary"
      ],
      "metadata": {
        "id": "fwUtjPycQ6Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('fullDataset.csv')\n",
        "# Shuffles the dataset, as it was ordered by label during construction\n",
        "df = df.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "k9nZMJZTQ6qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converts all \"urgent\" labels to \"relevant\"\n",
        "# Allows us to collpase the trinary classification task into a relevant/irrelevant binary classification task\n",
        "def changeLabels(x):\n",
        "  if x == 2:\n",
        "    return 1\n",
        "  else:\n",
        "    return x"
      ],
      "metadata": {
        "id": "rDzS-LgFQ8RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define our docs based on the META flag\n",
        "if META:\n",
        "  docs = df[\"Meta\"].tolist()\n",
        "else:\n",
        "  docs = df[\"Full\"].tolist()\n",
        "\n",
        "# define our labels based on the THREE_CLASSES flags\n",
        "if THREE_CLASSES:\n",
        "  labels = df[\"Label\"].tolist()\n",
        "else:\n",
        "  labels = df['Label'].apply(changeLabels).tolist()\n",
        "\n",
        "print(len(docs))\n",
        "print(len(labels))"
      ],
      "metadata": {
        "id": "0KHNpTUqQ90k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all documents to Tf-Idf vectors\n",
        "tfidfVectorizer = TfidfVectorizer(strip_accents='unicode', lowercase = True, stop_words='english', max_features=7500)\n",
        "processed = tfidfVectorizer.fit_transform(docs)\n",
        "docTermMatrix = processed.toarray()"
      ],
      "metadata": {
        "id": "TFdPWfzFRccr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate train, test, validation sets\n",
        "train, validAndTest, trainLabels, validAndTestLabels = train_test_split(docTermMatrix, labels, test_size=0.3, random_state=RANDOM_STATE)\n",
        "valid, test, validLabels, testLabels = train_test_split(validAndTest, validAndTestLabels, test_size=0.5, random_state=RANDOM_STATE)"
      ],
      "metadata": {
        "id": "HzzgNcteSYpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of possible values for hyperparameters - we will grid search across all of these values\n",
        "NUM_TREES = [50, 75, 100, 125, 150] # number of decision trees in the forest\n",
        "CRITERION = [\"gini\", \"entropy\", \"log_loss\"] # function to determine homegeneity\n",
        "MIN_SAMPLES_SPLIT = [2, 3, 4] # minimum number of samples required to split a node\n",
        "MAX_FEATURES = [\"sqrt\", \"log2\"] # function to consider the number of features to look for when looking for a split"
      ],
      "metadata": {
        "id": "KL9UNyrKUh2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We grid search to optimize macro-averaged F1 on the validation set\n",
        "\n",
        "# Best validation f1/hyperparameters we've seen so far\n",
        "bestF1 = 0\n",
        "bestTrees = 0\n",
        "bestCriterion = \"\"\n",
        "bestMinSamplesSplit = 0\n",
        "bestMaxFeatures = \"\"\n",
        "\n",
        " # grid search\n",
        "for tree in NUM_TREES:\n",
        "  for criterion in CRITERION:\n",
        "    for split in MIN_SAMPLES_SPLIT:\n",
        "      for features in MAX_FEATURES:\n",
        "        # train a random forest classifier\n",
        "        forest = RandomForestClassifier(n_estimators=tree, criterion=criterion, min_samples_split=split, max_features=features, random_state = RANDOM_STATE)\n",
        "        forest.fit(train, trainLabels)\n",
        "\n",
        "        # evaluation on the validation set\n",
        "        predictions = forest.predict(valid)\n",
        "        f1 = f1_score(validLabels, predictions, average=\"macro\")\n",
        "\n",
        "        # if we get a better validation f1, update the best f1/hyperparameters we've seen\n",
        "        if f1 > bestF1:\n",
        "          bestF1 = f1\n",
        "          bestTrees = tree\n",
        "          bestCriterion = criterion\n",
        "          bestMinSamplesSplit = split\n",
        "          bestMaxFeatures = features\n",
        "\n",
        "# best performing hyperparameters\n",
        "print(\"Best f1: \" + str(f1))\n",
        "print(\"Best number of trees: \" + str(bestTrees))\n",
        "print(\"Best criterion: \" + str(bestCriterion))\n",
        "print(\"Best minimum number samples to split a tree: \" + str(bestMinSamplesSplit))\n",
        "print(\"Best max features determiner: \" + str(bestMaxFeatures))"
      ],
      "metadata": {
        "id": "T4MTmwQ1TFc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train a random forest classifier on the best hyperparameters we saw\n",
        "forest = RandomForestClassifier(n_estimators=bestTrees, criterion=bestCriterion, min_samples_split=bestMinSamplesSplit, max_features=bestMaxFeatures)\n",
        "forest.fit(train, trainLabels)"
      ],
      "metadata": {
        "id": "vGfYLGPhXKiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate that random forest classifier on the test set\n",
        "predictions = forest.predict(test)\n",
        "print(classification_report(testLabels, predictions))"
      ],
      "metadata": {
        "id": "w_z6VzX3THP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(testLabels, predictions))"
      ],
      "metadata": {
        "id": "JvBO6V8IPASG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}